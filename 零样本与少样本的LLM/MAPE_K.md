# 利用大型语言模型进行微服务应用的自动修复：实验研究

## 1. 论文解决的核心问题

这篇论文主要解决了**微服务应用的运行时自动修复问题**。随着企业从传统的本地部署模型转向云平台和微服务架构，系统变得越来越复杂，导致生产环境中的故障增加。尽管在自治计算和自适应计算领域有持续努力，但很多云服务和应用仍然会遇到需要手动干预的故障，这些故障通常是因为：

1. 在设计阶段没有考虑到这些故障及其修复方法
2. 故障修复步骤的规划和执行需要使用运行时上下文信息的复杂工作流

特别是，虽然自动事件检测和优先级排序等方面已经实现了自动化，但操作人员仍需投入大量手动工作编写修复脚本，这是因为云服务环境中微服务数量庞大且复杂：
- Netflix拥有1000多个微服务
- Uber拥有4000多个微服务
- 服务更新频繁，导致相同异常在不同服务中表现不同
- 为类似异常编写具有不同配置的修复脚本是费时且重复的任务

## 2. 提出的解决方案

作者提出利用大型语言模型(LLMs)来**生成并执行Ansible playbook**，以自动解决微服务环境中的问题。具体解决方案包括：

1. 使用上下文学习(in-context learning)方法对预训练的LLMs进行调优
2. 构建特定的KubePlaybook数据集用于调优LLMs
3. 让调优后的LLMs能够根据自然语言提示生成Ansible脚本
4. 将生成的脚本自动执行以解决微服务环境中的故障

## 3. 研究贡献

论文的主要贡献包括：

1. 首次实验性研究LLMs用于生成微服务事件修复的Ansible playbook
2. 评估LLMs从自然语言提示生成Ansible脚本的有效性（无需代码输入）
3. 对比评估了LLaMa-2-70B与GPT-4模型在本地部署环境中生成Ansible代码的效果
4. 在未见过的应用(如Robot-shop、Sockshop和QOTD)和环境(如OpenShift)上测试了调优后的LLM
5. 实验证明模型在功能正确性方面可达到95.45%，平均正确性可达98.86%

## 4. 方法与架构详解

### 4.1 MAPE-K循环架构

作者将LLM基于的修复集成到MAPE-K循环架构中，特别支持循环的生成和执行阶段：

![LLM集成的MAPE-k循环架构](图1)

架构组件包括：

1. **任务代码库(TCR)**：用于LLMs的上下文调优，包含Ansible代码示例和对应的自然语言提示
2. **观察(Observe)**：监控微服务的指标、日志、事件、跟踪和告警，存储在症状库中
3. **分析和计划(Analyze & Plan)**：操作人员识别模式、检测异常和根本原因，制定修复计划
4. **生成和执行(Generate & Act)**：使用LLM将自然语言计划转换为Ansible代码并执行

### 4.2 提示工程与少量样本学习

作者设计了一个精心的提示模板，包含三个关键组件：

1. **系统提示(System)**：为模型提供身份和上下文指令，确保它专注于生成Ansible playbook
2. **操作员查询输入(OQI)**：包含问题的详细描述、特定条件、期望结果和约束
3. **Ansible Playbook代码(APC)**：与OQI对应的解决方案示例

提示模板结构：
```
System: Write an Ansible playbook following these instructions:

Example 1: OQI: Root Cause Action with configuration details.
Ansible playbook code that solves the root cause (APC).

Example 2: ...

Example n: ...

OQI: Root Cause Action with configuration details.
LLM output: Generated Ansible playbook that remediates the root cause.
```

### 4.3 任务代码库(TCR)构建

作者使用了专门设计的Kubeplaybook库，包含130个独特的Ansible playbook，分为三类：

1. **K8s基本命令playbook**（占62.3%）：主要设计用于K8s环境中的基本配置和部署任务
2. **从开源库收集的Ansible Playbook**（占19.2%）：从Galaxy和GitHub等平台收集
3. **混沌工程修复playbook**（占18.5%）：处理通过混沌工程产生的操作故障

### 4.4 评估指标

作者使用两个关键性能指标评估方法的有效性：

1. **功能正确性指标(FC)**：使用pass@k指标，如果k个代码样本中至少有一个通过指定测试，则认为问题得到解决

2. **平均正确性指标(AC)**：作者设计的评估指标，用于评估单个Ansible脚本中生成的代码块或任务的质量：

   ```
   AC = (Σⁿₘⱼ₌₁ Σⁿᵢ₌₁ (Cᵢ/Nᵢ * 100)) / m * 100
   ```

   其中:
   - Cᵢ：每个APC中通过测试的正确任务数
   - Nᵢ：该APC中的总任务数
   - m：APC的总数

## 5. 实验设计与结果

### 5.1 RQ1：超参数调优的影响

实验方法：
- 使用相同条件比较GPT-4和LLaMa-2-70B
- 设置Top-p=1，使用10个示例进行少量样本学习
- Temperature参数从0.0到1.0变化
- 使用FC和AC指标进行全面验证

实验结果：
- Temperature=0.6时获得最佳结果：
  - GPT-4：FC=88.89%，AC=95.83%
  - LLaMa-2-70B：FC=70%，AC=90.01%
- 极低(0.0)或极高(1.0)的Temperature降低性能并引入更多随机性

### 5.2 RQ2：不同学习方法的比较

实验方法：
- 比较零样本、单样本和少量样本学习方法
- 使用确定的最优Temperature=0.6
- 对10个未见过的示例评估模型

实验结果：
| 学习方法 | 模型 | FC pass@1 | AC |
|---------|------|-----------|-----|
| 零样本 | LLaMa-2-70B | 0% | 0% |
| 零样本 | GPT-4 | 12.50% | 27.08% |
| 单样本 | LLaMa-2-70B | 30% | 40% |
| 单样本 | GPT-4 | 31.25% | 62.50% |
| 少量样本 | LLaMa-2-70B | 70% | 90.01% |
| 少量样本 | GPT-4 | 88.89% | 95.83% |

结论：
- 两种模型在零样本方法上表现不佳
- 单样本显著提升性能
- 少量样本(10个示例)大幅提高适应性，GPT-4达到95.83%准确率

### 5.3 RQ3：调优模型的泛化能力

实验方法：
- 使用确定的最优设置(Few-shot, T=0.6)
- 测试模型生成Kubectl命令和处理实时故障的能力
- 扩展测试集：71个Kubectl命令和24个实时故障
- 评估模型对未见过环境(OpenShift)和应用(QOTD)的适应性

实验结果：
| 使用场景 | 模型 | FC pass@1 | AC |
|---------|------|-----------|-----|
| 实时故障 | LLaMa-2-70B | 62.5% | 75% |
| 实时故障 | GPT-4 | 88% | 92.36% |
| Kubectl命令 | LLaMa-2-70B | 77.27% | 95% |
| Kubectl命令 | GPT-4 | 95.45% | 98.86% |

泛化能力：
- 两个模型都能适应新的未见过环境(OpenShift)
- 能够使用OC shell命令获取环境信息来修复故障
- 生成的Ansible playbook经过手动测试，成功解决了识别的问题

## 6. 算法与流程

作者特别提出了一个用于计算功能正确性的算法：

```
算法1: 功能正确性
输入: 参数n: playbook总数
     参数c: 正确playbook数
     参数k: pass@k中的K
输出: pass@k
1 如果 (n-c < k) 则
2    返回 1.0
3 否则
4    返回 1.0 - np.prod(1.0 - k/np.arange(n-c+1, n+1))
5 结束
```

该算法通过考虑从剩余playbook中获得至少k个正确playbook的机会，帮助估计达到所需成功率的概率。

## 7. 优势与局限性

优势：
1. 减少操作人员在解决事件中的手动工作
2. 提高事件缓解过程的效率
3. 能够适应不同环境、应用和主机
4. 少量样本学习实现有效代码生成

局限性：
1. 生产环境中可能包含敏感信息，不适合使用公共LLMs
2. 需要操作员参与根因分析、提示调整和代码验证
3. LLMs需要少量样本调优以适应特定平台和修复方法

## 8. 结论与未来工作

该研究表明，使用LLMs进行微服务自动修复是可行的，特别是GPT-4和LLaMa-2-70B模型在生成Ansible playbook方面表现出色，分别达到了98.86%和95%的平均正确率。

未来工作方向：
1. 扩展自动化提示构建，通过事件、日志和指标分析
2. 包含严重性级别、历史数据和预测资源使用等因素
3. 识别故障模式并集成反馈循环以持续增强修复效果
4. 通过添加第二个LLM层来增强当前架构，实现提示构建自动化

总之，这项研究为利用LLMs实现微服务自动修复提供了坚实的基础，在减少手动干预和提高系统可靠性方面具有显著潜力。